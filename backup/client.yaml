apiVersion: batch/v1
kind: Job
metadata:
  name: triton-inference-client
spec:
  template:
    spec:
      containers:
        - name: triton-inference-client
          image: nvcr.io/nvidia/tritonserver:23.04-py3-sdk
          command: ["sh", "-c", "echo 'Hello, World!' && sleep 7200"]
          resources:
            limits:
              memory: 2Gi
            requests:
              memory: 2Gi
      restartPolicy: Never
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: "karpenter.sh/capacity-type"
                    operator: "In"
                    values: ["spot"]
  backoffLimit: 1
