apiVersion: batch/v1
kind: Job
metadata:
  name: triton-inference-server
spec:
  template:
    spec:
      containers:
        - name: triton-inference-server
          image: cmpark0126/eks-triton-test:23.04-py3
          # command: ["sh", "-c", "echo 'Hello, World!' && sleep 7200"]
          command: ["/bin/bash", "-c"]
          args:
            - |
              python tutorials/Quick_Deploy/PyTorch/export.py
              mv model.pt models/resnet50/1/model.pt
              tritonserver --model-repository models --allow-metrics true --allow-gpu-metrics true
          resources:
            limits:
              memory: 10Gi
              nvidia.com/gpu: "1"
            requests:
              memory: 10Gi
              nvidia.com/gpu: "1"
      restartPolicy: Never
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: "node.kubernetes.io/instance-type"
                    operator: "In"
                    values: ["g4dn.xlarge"] # g4dn.xlarge for tesla t4, g5.xlarge for a10g
                  - key: "karpenter.sh/capacity-type"
                    operator: "In"
                    values: ["spot"]
  backoffLimit: 1
