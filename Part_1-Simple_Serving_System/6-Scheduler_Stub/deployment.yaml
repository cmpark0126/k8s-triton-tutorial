apiVersion: apps/v1
kind: Deployment
metadata:
  name: server-1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: server-1
  template:
    metadata:
      labels:
        app: server-1
    spec:
      serviceAccountName: s3-full-access-role
      containers:
          - name: triton-inference-client
            image: python:3.9
            command: ["/bin/bash", "-c"]
            # 0.0.0.0 은 모든 IPv4 주소에서 오는 연결을 수용하겠다는 의미
            args: [ "git clone https://github.com/cmpark0126/ml-serving-system-building && pip install -r ml-serving-system-building/Part_1-Simple_Serving_System/6-Scheduler_Stub/requirements.txt && mv ml-serving-system-building/Part_1-Simple_Serving_System/6-Scheduler_Stub/main.py main.py && uvicorn main:app --host 0.0.0.0 --port 8000" ]
            resources:
              limits:
                cpu: "1"
                memory: 256Mi
              requests:
                cpu: "1"
                memory: 256Mi
            ports:
              - containerPort: 8000
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: "karpenter.sh/capacity-type"
                    operator: "In"
                    values: ["spot"]
